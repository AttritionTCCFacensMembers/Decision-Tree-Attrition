{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Import das bibliotécas utilizadas na etapa de avaliação de hiperparametros e do desempenho de diversos modelos treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1634497691244,
     "user": {
      "displayName": "Eduardo Mourão",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjeId2_UOy4jSo0MdXdQT1rKHwUrGISgd4TPmD4gQ=s64",
      "userId": "00141677318147528315"
     },
     "user_tz": 180
    },
    "id": "m3YARLewZLLV"
   },
   "outputs": [],
   "source": [
    "#imports que serão usados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "import scikitplot as skplt\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Import do dataframe pós tratamento e limpeza dos dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4410 entries, 0 to 4409\n",
      "Data columns (total 28 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Age                      4410 non-null   int64  \n",
      " 1   Attrition                4410 non-null   int64  \n",
      " 2   BusinessTravel           4410 non-null   object \n",
      " 3   Department               4410 non-null   object \n",
      " 4   DistanceFromHome         4410 non-null   int64  \n",
      " 5   Education                4410 non-null   int64  \n",
      " 6   EducationField           4410 non-null   object \n",
      " 7   Gender                   4410 non-null   object \n",
      " 8   JobLevel                 4410 non-null   int64  \n",
      " 9   JobRole                  4410 non-null   object \n",
      " 10  MaritalStatus            4410 non-null   object \n",
      " 11  MonthlyIncome            4410 non-null   int64  \n",
      " 12  NumCompaniesWorked       4391 non-null   float64\n",
      " 13  PercentSalaryHike        4410 non-null   int64  \n",
      " 14  StockOptionLevel         4410 non-null   int64  \n",
      " 15  TotalWorkingYears        4401 non-null   float64\n",
      " 16  TrainingTimesLastYear    4410 non-null   int64  \n",
      " 17  YearsAtCompany           4410 non-null   int64  \n",
      " 18  YearsSinceLastPromotion  4410 non-null   int64  \n",
      " 19  YearsWithCurrManager     4410 non-null   int64  \n",
      " 20  EnvironmentSatisfaction  4385 non-null   float64\n",
      " 21  JobSatisfaction          4390 non-null   float64\n",
      " 22  WorkLifeBalance          4372 non-null   float64\n",
      " 23  JobInvolvement           4410 non-null   int64  \n",
      " 24  PerformanceRating        4410 non-null   int64  \n",
      " 25  worked_days_year         4410 non-null   int64  \n",
      " 26  holidays_worked          4410 non-null   int64  \n",
      " 27  delta_min_year           4410 non-null   float64\n",
      "dtypes: float64(6), int64(16), object(6)\n",
      "memory usage: 964.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Construção do modelo de Arvore de Decisões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliar o problema proposto de attrition, além da etapa de exploração de dados, onde conseguimos visualizar diretamente os principais fatores que influenciam attrition, também utilizaremos o modelo de Arvore de Decisões para classificar os funcionários que potencialmente podem deixar a companhia de forma voluntária. O objetivo do modelo é criar um processo preditivo que auxilie a tomada de decisão e a dê visibilidade para os potenciais casos de attrition. \n",
    "\n",
    "Além disso, um dos propósitos é também entender como o modelo da Arvore de Decisões se comporta em relação ao problema proposto (attrition), portanto, além da melhor precisão, também serão avaliados a otimização dos hiperparâmetros do modelo preditivo, utilizando-se a bibliotéca a função GridSearchCV da bibliotéca Scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 - Métricas de desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação do modelo de DecisionTree, utilizaremos algumas métricas de desempenho de modelos de classificação.\n",
    "\n",
    "Entre elas, temos: Precisão, Revocação, F-Medida e Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funções das métricas de desempenho\n",
    "\n",
    "# Matriz de Confusão\n",
    "def Matriz_Confusao (p_y_test, p_y_pred):\n",
    "    vn, fp, fn, vp = confusion_matrix(p_y_test, p_y_pred).ravel()\n",
    "    return vn, fp, fn, vp\n",
    "\n",
    "#Acuracia\n",
    "def calcacuracia (vp,fp,vn,fn):\n",
    "    acuracia = (vp+vn)/(vp+fp+vn+fn)\n",
    "    return acuracia\n",
    "\n",
    "#Precisao\n",
    "def calcprecisao(vp,fp,vn,fn):\n",
    "    precisao = vp / (vp + fp)\n",
    "    return precisao\n",
    "\n",
    "#Revocação\n",
    "def calcrevocacao(vp,fp,vn,fn):\n",
    "    revocacao = vp / (vp + fn)\n",
    "    return revocacao\n",
    "\n",
    "#F-Medida\n",
    "def calcfmedida(P,R):\n",
    "    fmedida = 2 * (P * R / (P + R))\n",
    "    return fmedida\n",
    "\n",
    "\n",
    "# Utilizando as funções de report do skplt.metrics\n",
    "def resultados(res):\n",
    "\n",
    "    # Medidas de acerto \n",
    "    for phase in res.keys():\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(\"Evaluating %s\" %(phase))\n",
    "        print(classification_report(res[phase][\"actual\"], res[phase][\"pred\"]))\n",
    "    \n",
    "    # Matriz de Confusão\n",
    "    for phase in res.keys():\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(\"Evaluating %s\" %(phase))\n",
    "        skplt.metrics.plot_confusion_matrix(res[phase][\"actual\"], res[phase][\"pred\"])\n",
    "        plt.show()\n",
    "    \n",
    "    # Curva ROC-AUC\n",
    "    for phase in res.keys():\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(\"Evaluating %s\" %(phase))\n",
    "        skplt.metrics.plot_roc_curve(res[phase][\"actual\"], res[phase][\"prob\"])\n",
    "        plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 - Avaliação de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi observado que alguns trabalhos na literatura, realizam a construção dos modelos separando os datasets por diferentes departamentos da companhia, visto que diferentes departamentos podem possuir caracteristicas totalmente diferentes em relação a vários fatores de performance, satisfação, workload, entre outros. \n",
    "\n",
    "Seguindo esta recomendação, serão separados os datasets em três cenários. \n",
    "\n",
    "O primeiro cenário tem como propósito direcionar a construção de um modelo que atenda a necessidade de toda a companhia. Portanto, serão utilizados todos os dados. O segundo e o terceiro cenário, utilizaram respectivamente os dados apenas do departamento de Sales e de R&D, visto que são os departamentos que possuem maior volume de dados. O departamento de RH possui uma quantidade muito pequena de dados, portanto, não faria sentido construir um modelo específico para o departamento. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separação dos dados para train/test de diferentes modelos, considerando: Dataset inteiro e separado por dois departamentos\n",
    "\n",
    "X_Sales = df[(df['Department']=='Sales')][df.columns.drop('Attrition')]\n",
    "y_sales = df[(df['Department']=='Sales')][['Attrition']]\n",
    "\n",
    "X_RandD = df[(df['Department']=='Research & Development')][df.columns.drop('Attrition')]\n",
    "y_RandD = df[(df['Department']=='Research & Development')][['Attrition']]\n",
    "\n",
    "X_all = df[df.columns.drop('Attrition')]\n",
    "y_all = df[['Attrition']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para otimizar o processo de avaliação dos melhores hiperparametros da Arvore de Descisão, construímos um algoritmo utilizando Pipeline e Gridsearch, o qual realiza um loop através de um conjunto de combinações de diferentes diferentes parâmetros, tanto no pré-processamento (estratégia de tratamento de valores nulos), como nos hiperparâmetros da Arvore de Descisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 - Primeiro batch de testes com diferentes hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em testes iniciais, verificamos que quando as diferentes ranges de max_depth da Arvore de Descisão eram passados diretamente no GridSearch, estavam sendo selecionados apenas o resultado maior de max_depth, o que deixava o modelo overfitado. Portanto, nas interações criadas, forçamos o GridSearch a sempre adotar um valor específico de max_depth, que foi interado com todas as outras possibilidades de escolha dos outros parâmetros. \n",
    "\n",
    "OBS: O código abaixo está comentado, pois demorou aproximandamente 24 horas para executar, o que acabou tirando um pouco o propósito da tentativa de otimizar o processo, mas nos ajudou a afunilar as escolhas e os ranges para uma próxima rodada de testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "test_size = [0.15,0.20,0.25]\n",
    "SIStrategyCat = 'most_frequent'\n",
    "SIStrategyNum = ['mean','median','most_frequent']\n",
    "OneHotStrat = 'ignore'\n",
    "GSscoring=['precision', 'recall','f1']\n",
    "max_depth=[[5],[10],[20],[25],[40],[50],[75],[100]]\n",
    "train_test_data = [[X_all,y_all],[X_Sales,y_sales],[X_RandD,y_RandD]]\n",
    "\n",
    "#Nomeando os dataframes\n",
    "X_all.name = 'X_all'\n",
    "y_all.name = 'y_all'\n",
    "X_Sales.name = 'X_Sales'\n",
    "y_sales.name = 'y_sales'\n",
    "X_RandD.name = 'X_RandD'\n",
    "y_RandD.name = 'y_RandD'\n",
    "\n",
    "            \n",
    "#Separação das colunas numéricas continuas e categóricas que receberão diferentes tratativas no pipeline\n",
    "numeric_features = df.drop(columns=['Attrition']).select_dtypes(exclude=[object]).columns.values.tolist()\n",
    "categorical_features = df.select_dtypes(include=[object]).columns.values.tolist()\n",
    "\n",
    "#Dicionário para armazenar os resultados\n",
    "resumo_resultados = {\n",
    "            'VP': [],\n",
    "            'FP': [],\n",
    "            'VN': [],\n",
    "            'FN': [],\n",
    "            'Acuracia'  : [],\n",
    "            'Precisao'  : [], \n",
    "            'Revocação' : [],\n",
    "            'F-Medida'  : [],\n",
    "            'ParametrosArvore': [],\n",
    "            'data':[],\n",
    "            'Test_size': [],\n",
    "            'SIStrategyNum': [],\n",
    "            'OneHotStrat':[],\n",
    "            'GSscoring':[],\n",
    "            'max_depth':[]\n",
    "            \n",
    "                     }\n",
    "\n",
    "for data in train_test_data:\n",
    "    for size in test_size:\n",
    "        for numstrat in SIStrategyNum:\n",
    "            for scoringmethod in GSscoring:\n",
    "                for depth in max_depth:\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(data[0],data[1], test_size=size, random_state=42)\n",
    "\n",
    "\n",
    "                    cnts_pipeline = Pipeline([\n",
    "                        ('impute', SimpleImputer(strategy=numstrat)),\n",
    "                        ('scale', StandardScaler())])\n",
    "\n",
    "                    categ_pipeline = Pipeline([\n",
    "                        ('impute', SimpleImputer(strategy=SIStrategyCat)),\n",
    "                        ('onehot', OneHotEncoder(handle_unknown=OneHotStrat))])\n",
    "\n",
    "                    preprocess_pipeline = ColumnTransformer([\n",
    "                        ('continuous', cnts_pipeline, numeric_features),\n",
    "                        ('cat', categ_pipeline, categorical_features)])\n",
    "\n",
    "                    pipeline = Pipeline(steps=[(\"preprocessor\", preprocess_pipeline), (\"classifier\", DecisionTreeClassifier())])\n",
    "\n",
    "                    param_grid = [{\n",
    "                               'classifier__criterion': ['gini','entropy'],\n",
    "                               'classifier__splitter': ['best', 'random'],\n",
    "                               'classifier__max_depth': depth,\n",
    "                               'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "                               'classifier__min_samples_split':range(1,10),\n",
    "                               'classifier__min_samples_leaf': range(1,10)}]\n",
    "\n",
    "                    model_gs = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoringmethod)\n",
    "                    model_gs.fit(X_train, y_train)\n",
    "\n",
    "                    print(\"________________________________________________________________\")\n",
    "                    print('Modelo/Dados =',data[0].name,data[1].name)\n",
    "                    print(\"Combinação de Parâmetros:\")\n",
    "                    print('Test size =',size,'NumericInputer =',numstrat,'GSscoring =',scoringmethod, 'Max_depth =',depth)\n",
    "                    print(\"*********************\\n\")\n",
    "                    print(\"Resultados gerais:\")\n",
    "                    print('Best Score: ',model_gs.best_score_)\n",
    "                    print('Melhores parâmetros da árvore: ',model_gs.best_params_)\n",
    "                    print(\"*********************\\n\")\n",
    "                    print(\"Matriz de Confusão:\")\n",
    "                    Results = {\"train\": {\"actual\": y_train,\n",
    "                               \"pred\": model_gs.predict(X_train),\n",
    "                               \"prob\": model_gs.predict_proba(X_train)},\n",
    "\n",
    "                       \"test\": {\"actual\": y_test,\n",
    "                               \"pred\": model_gs.predict(X_test),\n",
    "                               \"prob\": model_gs.predict_proba(X_test)}}\n",
    "\n",
    "                    resultados(Results)\n",
    "                    print(\"________________________________________________________________\")\n",
    "\n",
    "                    y_pred = model_gs.predict(X_test)\n",
    "\n",
    "                    vn, fp, fn, vp = Matriz_Confusao (y_test, y_pred)\n",
    "                    Acuracia = calcacuracia(vp,fp,vn,fn)\n",
    "                    P = calcprecisao(vp,fp,vn,fn)\n",
    "                    R = calcrevocacao(vp,fp,vn,fn)\n",
    "                    FMedida = calcfmedida(P,R)\n",
    "\n",
    "                    resumo_resultados['VP'].append(vp)\n",
    "                    resumo_resultados['FP'].append(fp)\n",
    "                    resumo_resultados['VN'].append(vn)\n",
    "                    resumo_resultados['FN'].append(fn)                \n",
    "                    resumo_resultados['Acuracia'].append(Acuracia)\n",
    "                    resumo_resultados['Precisao'].append(P)                \n",
    "                    resumo_resultados['Revocação'].append(R)\n",
    "                    resumo_resultados['F-Medida'].append(FMedida)                             \n",
    "                    resumo_resultados['ParametrosArvore'].append(model_gs.best_params_)\n",
    "                    resumo_resultados['data'].append([data[0].name,data[1].name])\n",
    "                    resumo_resultados['Test_size'].append(size)\n",
    "                    resumo_resultados['SIStrategyNum'].append(numstrat)\n",
    "                    resumo_resultados['OneHotStrat'].append(OneHotStrat)\n",
    "                    resumo_resultados['GSscoring'].append(scoringmethod)\n",
    "                    resumo_resultados['max_depth'].append(depth)\n",
    "                    \n",
    "\n",
    "                    resultados_gerais = pd.DataFrame(resumo_resultados)\n",
    "\n",
    "resultados_gerais.to_csv('resultados_gerais.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VP</th>\n",
       "      <th>FP</th>\n",
       "      <th>VN</th>\n",
       "      <th>FN</th>\n",
       "      <th>Acuracia</th>\n",
       "      <th>Precisao</th>\n",
       "      <th>Revocação</th>\n",
       "      <th>F-Medida</th>\n",
       "      <th>ParametrosArvore</th>\n",
       "      <th>data</th>\n",
       "      <th>Test_size</th>\n",
       "      <th>SIStrategyNum</th>\n",
       "      <th>OneHotStrat</th>\n",
       "      <th>GSscoring</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>562</td>\n",
       "      <td>90</td>\n",
       "      <td>0.858006</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>556</td>\n",
       "      <td>37</td>\n",
       "      <td>0.929003</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.614583</td>\n",
       "      <td>0.715152</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>8</td>\n",
       "      <td>558</td>\n",
       "      <td>11</td>\n",
       "      <td>0.971299</td>\n",
       "      <td>0.913978</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.899471</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 20, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "      <td>553</td>\n",
       "      <td>4</td>\n",
       "      <td>0.974320</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.915423</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 25, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>563</td>\n",
       "      <td>5</td>\n",
       "      <td>0.987915</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 40, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90</td>\n",
       "      <td>9</td>\n",
       "      <td>557</td>\n",
       "      <td>6</td>\n",
       "      <td>0.977341</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 50, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>561</td>\n",
       "      <td>6</td>\n",
       "      <td>0.983384</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.942408</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 75, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>563</td>\n",
       "      <td>6</td>\n",
       "      <td>0.986405</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 100, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>554</td>\n",
       "      <td>77</td>\n",
       "      <td>0.865559</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.299213</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 6, 'classifier__min_samples_split': 4, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>23</td>\n",
       "      <td>543</td>\n",
       "      <td>39</td>\n",
       "      <td>0.906344</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mean</td>\n",
       "      <td>ignore</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VP  FP   VN  FN  Acuracia  Precisao  Revocação  F-Medida  \\\n",
       "0  6   4   562  90  0.858006  0.600000  0.062500   0.113208   \n",
       "1  59  10  556  37  0.929003  0.855072  0.614583   0.715152   \n",
       "2  85  8   558  11  0.971299  0.913978  0.885417   0.899471   \n",
       "3  92  13  553  4   0.974320  0.876190  0.958333   0.915423   \n",
       "4  91  3   563  5   0.987915  0.968085  0.947917   0.957895   \n",
       "5  90  9   557  6   0.977341  0.909091  0.937500   0.923077   \n",
       "6  90  5   561  6   0.983384  0.947368  0.937500   0.942408   \n",
       "7  90  3   563  6   0.986405  0.967742  0.937500   0.952381   \n",
       "8  19  12  554  77  0.865559  0.612903  0.197917   0.299213   \n",
       "9  57  23  543  39  0.906344  0.712500  0.593750   0.647727   \n",
       "\n",
       "                                                                                                                                                                                                 ParametrosArvore  \\\n",
       "0  {'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}    \n",
       "1  {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}        \n",
       "2  {'classifier__criterion': 'entropy', 'classifier__max_depth': 20, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}   \n",
       "3  {'classifier__criterion': 'gini', 'classifier__max_depth': 25, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}        \n",
       "4  {'classifier__criterion': 'gini', 'classifier__max_depth': 40, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}      \n",
       "5  {'classifier__criterion': 'entropy', 'classifier__max_depth': 50, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}     \n",
       "6  {'classifier__criterion': 'entropy', 'classifier__max_depth': 75, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}     \n",
       "7  {'classifier__criterion': 'entropy', 'classifier__max_depth': 100, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}    \n",
       "8  {'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 6, 'classifier__min_samples_split': 4, 'classifier__splitter': 'best'}      \n",
       "9  {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 3, 'classifier__splitter': 'best'}        \n",
       "\n",
       "                 data  Test_size SIStrategyNum OneHotStrat  GSscoring  \\\n",
       "0  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "1  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "2  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "3  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "4  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "5  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "6  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "7  ['X_all', 'y_all']  0.15       mean          ignore      precision   \n",
       "8  ['X_all', 'y_all']  0.15       mean          ignore      recall      \n",
       "9  ['X_all', 'y_all']  0.15       mean          ignore      recall      \n",
       "\n",
       "   max_depth  \n",
       "0  0.15       \n",
       "1  0.15       \n",
       "2  0.15       \n",
       "3  0.15       \n",
       "4  0.15       \n",
       "5  0.15       \n",
       "6  0.15       \n",
       "7  0.15       \n",
       "8  0.15       \n",
       "9  0.15       "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_exemplo1 = pd.read_csv('resultados_gerais.csv')\n",
    "resultados_exemplo1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após realizar o primeiro batch de testes, entendemos que seria necessário refinar as interações, visto que poderiamos eliminar alguns elimentos que foram interados.\n",
    "\n",
    "Abaixo, um breve resumo das principais conclusões:\n",
    "\n",
    " * Não houve variação significativa em relação ao test size, portanto, adotamos como padrão o valor mais indicado na literatura (80/20 split). \n",
    " * Chegamos à conclusão que o melhor método de scoring para o nosso problema, seria o f1, visto que o mesmo representa um equilibrio entre precision e recall. E no problema que estamos buscando resolver, um cenário de equilibrio é o mais desejado. \n",
    " * Max_depth acima de 20 induz o modelo ao overfitting. E mesmo o valor 20 em algumas combinações, também ocasionou overfitting.\n",
    " * Após uma rodada de testes manuais (executando o modelo individualmente com os hiperparâmetros sinalizados pelo GridSearch), entendemos que seria necessário também olhar mais de perto as diferenças entre os resultados de treino e teste. Assim como no exemplo de max_depth acima de 20, encontramos alguns outros exemplos de overfitting quando olhamos também para os dados de treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 - Segundo batch de testes com diferentes hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o segundo batch de testes, além dos pontos levantados acima, também buscamos na literatura algumas alternativas para controlar o overfitting.\n",
    "\n",
    "Um delas, foi forçar a variação de sample_leaf e sample_split. Na literatura, encontramos referências indicando testes no valor entre 2 a aproximadamente 40 para sample_split e 1 entre aproximadamente 20, para sample_leaf.\n",
    "\n",
    "Foi também reduzido o range de max_depth, com o objetivo de avaliar o range entre 10 e 20.\n",
    "\n",
    "E por fim, incluímos nas interações, os testes com os dados de treino, para avaliar de maneira mais dinâmica possíveis overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Combinação de parâmetros/hiperparâmetros a serem testados no treinamento do modelo\n",
    "SIStrategyNum = ['mean','median','most_frequent']\n",
    "max_depth=[[10],[13],[15],[16],[17],[18],[19],[20]]\n",
    "train_test_data = [[X_all,y_all],[X_Sales,y_sales],[X_RandD,y_RandD]]\n",
    "\n",
    "sample_split = []\n",
    "for i in range(2,41,6):\n",
    "    sample_split.append([i])\n",
    "\n",
    "sample_leaf =  []\n",
    "for i in range(1,20,3):\n",
    "    sample_leaf.append([i])\n",
    "\n",
    "\n",
    "#Nomeando os dataframes\n",
    "X_all.name = 'X_all'\n",
    "y_all.name = 'y_all'\n",
    "X_Sales.name = 'X_Sales'\n",
    "y_sales.name = 'y_sales'\n",
    "X_RandD.name = 'X_RandD'\n",
    "y_RandD.name = 'y_RandD'\n",
    "\n",
    "            \n",
    "#Separação das colunas numéricas continuas e categóricas que receberão diferentes tratativas no pipeline\n",
    "numeric_features = df.drop(columns=['Attrition']).select_dtypes(exclude=[object]).columns.values.tolist()\n",
    "categorical_features = df.select_dtypes(include=[object]).columns.values.tolist()\n",
    "\n",
    "#Dicionário para armazenar os resultados\n",
    "resumo_resultados = {\n",
    "            'VP_teste': [],\n",
    "            'FP_teste': [],\n",
    "            'VN_teste': [],\n",
    "            'FN_teste': [],\n",
    "            'Acuracia_teste'  : [],\n",
    "            'Precisao_teste'  : [], \n",
    "            'Revocação_teste' : [],\n",
    "            'F-Medida_teste'  : [],\n",
    "            'ParametrosArvore': [],\n",
    "            'data':[],\n",
    "            'SIStrategyNum': [],\n",
    "            'max_depth':[],\n",
    "            'VP_treino': [],\n",
    "            'FP_treino': [],\n",
    "            'VN_treino': [],\n",
    "            'FN_treino': [],\n",
    "            'Acuracia_treino'  : [],\n",
    "            'Precisao_treino'  : [], \n",
    "            'Revocação_treino' : [],\n",
    "            'F-Medida_treino'  : []\n",
    "                     }\n",
    "\n",
    "for data in train_test_data:\n",
    "    for numstrat in SIStrategyNum:\n",
    "        for depth in max_depth:\n",
    "            for split in sample_split:\n",
    "                for leaf in sample_leaf:\n",
    "            \n",
    "                    X_train, X_test, y_train, y_test = train_test_split(data[0],data[1], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "                    cnts_pipeline = Pipeline([\n",
    "                        ('impute', SimpleImputer(strategy=numstrat)),\n",
    "                        ('scale', StandardScaler())])\n",
    "\n",
    "                    categ_pipeline = Pipeline([\n",
    "                        ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "                        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "                    preprocess_pipeline = ColumnTransformer([\n",
    "                        ('continuous', cnts_pipeline, numeric_features),\n",
    "                        ('cat', categ_pipeline, categorical_features)])\n",
    "\n",
    "                    pipeline = Pipeline(steps=[(\"preprocessor\", preprocess_pipeline), (\"classifier\", DecisionTreeClassifier())])\n",
    "\n",
    "                    param_grid = [{\n",
    "                               'classifier__criterion': ['gini','entropy'],\n",
    "                               'classifier__splitter': ['best', 'random'],\n",
    "                               'classifier__max_depth': depth,\n",
    "                               'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "                               'classifier__min_samples_split':split,\n",
    "                               'classifier__min_samples_leaf': leaf\n",
    "                    }]\n",
    "\n",
    "                    model_gs = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "                    model_gs.fit(X_train, y_train)\n",
    "\n",
    "                    print(\"________________________________________________________________\")\n",
    "                    print('Modelo/Dados =',data[0].name,data[1].name)\n",
    "                    print(\"Combinação de Parâmetros:\")\n",
    "                    print('NumericInputer =',numstrat, 'Max_depth =',depth)\n",
    "                    print(\"*********************\\n\")\n",
    "                    print(\"Resultados gerais:\")\n",
    "                    print('Best Score: ',model_gs.best_score_)\n",
    "                    print('Melhores parâmetros da árvore: ',model_gs.best_params_)\n",
    "                    print(\"*********************\\n\")\n",
    "                    print(\"Matriz de Confusão:\")\n",
    "                    Results = {\"train\": {\"actual\": y_train,\n",
    "                               \"pred\": model_gs.predict(X_train),\n",
    "                               \"prob\": model_gs.predict_proba(X_train)},\n",
    "\n",
    "                       \"test\": {\"actual\": y_test,\n",
    "                               \"pred\": model_gs.predict(X_test),\n",
    "                               \"prob\": model_gs.predict_proba(X_test)}}\n",
    "\n",
    "                    resultados(Results)\n",
    "                    print(\"________________________________________________________________\")\n",
    "\n",
    "                    y_pred = model_gs.predict(X_test)\n",
    "\n",
    "                    vn, fp, fn, vp = Matriz_Confusao (y_test, y_pred)\n",
    "                    Acuracia = calcacuracia(vp,fp,vn,fn)\n",
    "                    P = calcprecisao(vp,fp,vn,fn)\n",
    "                    R = calcrevocacao(vp,fp,vn,fn)\n",
    "                    FMedida = calcfmedida(P,R)\n",
    "\n",
    "                    y_pred = model_gs.predict(X_train)\n",
    "\n",
    "                    vn_treino, fp_treino, fn_treino, vp_treino = Matriz_Confusao (y_train, y_pred)\n",
    "                    Acuracia_treino = calcacuracia(vp_treino,fp_treino,vn_treino,fn_treino)\n",
    "                    P_treino = calcprecisao(vp_treino,fp_treino,vn_treino,fn_treino)\n",
    "                    R_treino = calcrevocacao(vp_treino,fp_treino,vn_treino,fn_treino)\n",
    "                    FMedida_treino = calcfmedida(P_treino,R_treino)\n",
    "\n",
    "                    resumo_resultados['VP_teste'].append(vp)\n",
    "                    resumo_resultados['FP_teste'].append(fp)\n",
    "                    resumo_resultados['VN_teste'].append(vn)\n",
    "                    resumo_resultados['FN_teste'].append(fn)                \n",
    "                    resumo_resultados['Acuracia_teste'].append(Acuracia)\n",
    "                    resumo_resultados['Precisao_teste'].append(P)                \n",
    "                    resumo_resultados['Revocação_teste'].append(R)\n",
    "                    resumo_resultados['F-Medida_teste'].append(FMedida)                             \n",
    "                    resumo_resultados['ParametrosArvore'].append(model_gs.best_params_)\n",
    "                    resumo_resultados['data'].append([data[0].name,data[1].name])\n",
    "                    resumo_resultados['SIStrategyNum'].append(numstrat)\n",
    "                    resumo_resultados['max_depth'].append(depth)\n",
    "                    resumo_resultados['VP_treino'].append(vp_treino)\n",
    "                    resumo_resultados['FP_treino'].append(fp_treino)\n",
    "                    resumo_resultados['VN_treino'].append(vn_treino)\n",
    "                    resumo_resultados['FN_treino'].append(fn_treino)                \n",
    "                    resumo_resultados['Acuracia_treino'].append(Acuracia_treino)\n",
    "                    resumo_resultados['Precisao_treino'].append(P_treino)                \n",
    "                    resumo_resultados['Revocação_treino'].append(R_treino)\n",
    "                    resumo_resultados['F-Medida_treino'].append(FMedida_treino)                             \n",
    "\n",
    "\n",
    "                    resultados_gerais2 = pd.DataFrame(resumo_resultados)\n",
    "\n",
    "resultados_gerais2.to_csv('resultados_gerais2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, uma demonstração de como ficou o DF exportado com os resultados e os hiperparâmetros de cada interação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VP_teste</th>\n",
       "      <th>FP_teste</th>\n",
       "      <th>VN_teste</th>\n",
       "      <th>FN_teste</th>\n",
       "      <th>Acuracia_teste</th>\n",
       "      <th>Precisao_teste</th>\n",
       "      <th>Revocação_teste</th>\n",
       "      <th>F-Medida_teste</th>\n",
       "      <th>ParametrosArvore</th>\n",
       "      <th>data</th>\n",
       "      <th>SIStrategyNum</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>VP_treino</th>\n",
       "      <th>FP_treino</th>\n",
       "      <th>VN_treino</th>\n",
       "      <th>FN_treino</th>\n",
       "      <th>Acuracia_treino</th>\n",
       "      <th>Precisao_treino</th>\n",
       "      <th>Revocação_treino</th>\n",
       "      <th>F-Medida_treino</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>32</td>\n",
       "      <td>709</td>\n",
       "      <td>68</td>\n",
       "      <td>0.886621</td>\n",
       "      <td>0.695238</td>\n",
       "      <td>0.517730</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 4, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[10]</td>\n",
       "      <td>360</td>\n",
       "      <td>47</td>\n",
       "      <td>2911</td>\n",
       "      <td>210</td>\n",
       "      <td>0.927154</td>\n",
       "      <td>0.884521</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.736950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>16</td>\n",
       "      <td>725</td>\n",
       "      <td>33</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.815094</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 13, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[13]</td>\n",
       "      <td>464</td>\n",
       "      <td>36</td>\n",
       "      <td>2922</td>\n",
       "      <td>106</td>\n",
       "      <td>0.959751</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.814035</td>\n",
       "      <td>0.867290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>7</td>\n",
       "      <td>734</td>\n",
       "      <td>23</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>0.887218</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 15, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[15]</td>\n",
       "      <td>516</td>\n",
       "      <td>12</td>\n",
       "      <td>2946</td>\n",
       "      <td>54</td>\n",
       "      <td>0.981293</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.939891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>22</td>\n",
       "      <td>719</td>\n",
       "      <td>31</td>\n",
       "      <td>0.939909</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.805861</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 16, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[16]</td>\n",
       "      <td>545</td>\n",
       "      <td>14</td>\n",
       "      <td>2944</td>\n",
       "      <td>25</td>\n",
       "      <td>0.988946</td>\n",
       "      <td>0.974955</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.965456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>13</td>\n",
       "      <td>728</td>\n",
       "      <td>31</td>\n",
       "      <td>0.950113</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 17, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[17]</td>\n",
       "      <td>518</td>\n",
       "      <td>14</td>\n",
       "      <td>2944</td>\n",
       "      <td>52</td>\n",
       "      <td>0.981293</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.908772</td>\n",
       "      <td>0.940109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118</td>\n",
       "      <td>11</td>\n",
       "      <td>730</td>\n",
       "      <td>23</td>\n",
       "      <td>0.961451</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>0.874074</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 18, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[18]</td>\n",
       "      <td>537</td>\n",
       "      <td>8</td>\n",
       "      <td>2950</td>\n",
       "      <td>33</td>\n",
       "      <td>0.988379</td>\n",
       "      <td>0.985321</td>\n",
       "      <td>0.942105</td>\n",
       "      <td>0.963229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>123</td>\n",
       "      <td>8</td>\n",
       "      <td>733</td>\n",
       "      <td>18</td>\n",
       "      <td>0.970522</td>\n",
       "      <td>0.938931</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.904412</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 19, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[19]</td>\n",
       "      <td>567</td>\n",
       "      <td>0</td>\n",
       "      <td>2958</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994737</td>\n",
       "      <td>0.997361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>122</td>\n",
       "      <td>8</td>\n",
       "      <td>733</td>\n",
       "      <td>19</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.865248</td>\n",
       "      <td>0.900369</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 20, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>mean</td>\n",
       "      <td>[20]</td>\n",
       "      <td>550</td>\n",
       "      <td>2</td>\n",
       "      <td>2956</td>\n",
       "      <td>20</td>\n",
       "      <td>0.993764</td>\n",
       "      <td>0.996377</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.980392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>724</td>\n",
       "      <td>57</td>\n",
       "      <td>0.916100</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>{'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>median</td>\n",
       "      <td>[10]</td>\n",
       "      <td>416</td>\n",
       "      <td>39</td>\n",
       "      <td>2919</td>\n",
       "      <td>154</td>\n",
       "      <td>0.945295</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.729825</td>\n",
       "      <td>0.811707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92</td>\n",
       "      <td>9</td>\n",
       "      <td>732</td>\n",
       "      <td>49</td>\n",
       "      <td>0.934240</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.652482</td>\n",
       "      <td>0.760331</td>\n",
       "      <td>{'classifier__criterion': 'entropy', 'classifier__max_depth': 13, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}</td>\n",
       "      <td>['X_all', 'y_all']</td>\n",
       "      <td>median</td>\n",
       "      <td>[13]</td>\n",
       "      <td>448</td>\n",
       "      <td>14</td>\n",
       "      <td>2944</td>\n",
       "      <td>122</td>\n",
       "      <td>0.961451</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.785965</td>\n",
       "      <td>0.868217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VP_teste  FP_teste  VN_teste  FN_teste  Acuracia_teste  Precisao_teste  \\\n",
       "0  73        32        709       68        0.886621        0.695238         \n",
       "1  108       16        725       33        0.944444        0.870968         \n",
       "2  118       7         734       23        0.965986        0.944000         \n",
       "3  110       22        719       31        0.939909        0.833333         \n",
       "4  110       13        728       31        0.950113        0.894309         \n",
       "5  118       11        730       23        0.961451        0.914729         \n",
       "6  123       8         733       18        0.970522        0.938931         \n",
       "7  122       8         733       19        0.969388        0.938462         \n",
       "8  84        17        724       57        0.916100        0.831683         \n",
       "9  92        9         732       49        0.934240        0.910891         \n",
       "\n",
       "   Revocação_teste  F-Medida_teste  \\\n",
       "0  0.517730         0.593496         \n",
       "1  0.765957         0.815094         \n",
       "2  0.836879         0.887218         \n",
       "3  0.780142         0.805861         \n",
       "4  0.780142         0.833333         \n",
       "5  0.836879         0.874074         \n",
       "6  0.872340         0.904412         \n",
       "7  0.865248         0.900369         \n",
       "8  0.595745         0.694215         \n",
       "9  0.652482         0.760331         \n",
       "\n",
       "                                                                                                                                                                                                 ParametrosArvore  \\\n",
       "0  {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 4, 'classifier__splitter': 'best'}        \n",
       "1  {'classifier__criterion': 'entropy', 'classifier__max_depth': 13, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}     \n",
       "2  {'classifier__criterion': 'gini', 'classifier__max_depth': 15, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}        \n",
       "3  {'classifier__criterion': 'gini', 'classifier__max_depth': 16, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}        \n",
       "4  {'classifier__criterion': 'entropy', 'classifier__max_depth': 17, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}   \n",
       "5  {'classifier__criterion': 'entropy', 'classifier__max_depth': 18, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}     \n",
       "6  {'classifier__criterion': 'entropy', 'classifier__max_depth': 19, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}     \n",
       "7  {'classifier__criterion': 'entropy', 'classifier__max_depth': 20, 'classifier__max_features': 'auto', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}   \n",
       "8  {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}        \n",
       "9  {'classifier__criterion': 'entropy', 'classifier__max_depth': 13, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}     \n",
       "\n",
       "                 data SIStrategyNum max_depth  VP_treino  FP_treino  \\\n",
       "0  ['X_all', 'y_all']  mean          [10]      360        47          \n",
       "1  ['X_all', 'y_all']  mean          [13]      464        36          \n",
       "2  ['X_all', 'y_all']  mean          [15]      516        12          \n",
       "3  ['X_all', 'y_all']  mean          [16]      545        14          \n",
       "4  ['X_all', 'y_all']  mean          [17]      518        14          \n",
       "5  ['X_all', 'y_all']  mean          [18]      537        8           \n",
       "6  ['X_all', 'y_all']  mean          [19]      567        0           \n",
       "7  ['X_all', 'y_all']  mean          [20]      550        2           \n",
       "8  ['X_all', 'y_all']  median        [10]      416        39          \n",
       "9  ['X_all', 'y_all']  median        [13]      448        14          \n",
       "\n",
       "   VN_treino  FN_treino  Acuracia_treino  Precisao_treino  Revocação_treino  \\\n",
       "0  2911       210        0.927154         0.884521         0.631579           \n",
       "1  2922       106        0.959751         0.928000         0.814035           \n",
       "2  2946       54         0.981293         0.977273         0.905263           \n",
       "3  2944       25         0.988946         0.974955         0.956140           \n",
       "4  2944       52         0.981293         0.973684         0.908772           \n",
       "5  2950       33         0.988379         0.985321         0.942105           \n",
       "6  2958       3          0.999150         1.000000         0.994737           \n",
       "7  2956       20         0.993764         0.996377         0.964912           \n",
       "8  2919       154        0.945295         0.914286         0.729825           \n",
       "9  2944       122        0.961451         0.969697         0.785965           \n",
       "\n",
       "   F-Medida_treino  \n",
       "0  0.736950         \n",
       "1  0.867290         \n",
       "2  0.939891         \n",
       "3  0.965456         \n",
       "4  0.940109         \n",
       "5  0.963229         \n",
       "6  0.997361         \n",
       "7  0.980392         \n",
       "8  0.811707         \n",
       "9  0.868217         "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_exemplo2 = pd.read_csv('resultados_gerais2.csv')\n",
    "resultados_exemplo2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os melhores resultados foram inputadores utilizados na Parte 3 do trabalho."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Victor - Turnover predict and analysis_V4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
